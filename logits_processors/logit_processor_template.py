import torch
import json
from logits_processors.base_processor import BaseCustomProcessor
from scipy.stats import entropy

"""
Here is a template for your own custom logits processor. 
"""

class CustomProcessor(BaseCustomProcessor):
    def __init__(self, **kwargs):
        raise NotImplementedError(
            f"This is just a template."
        )
    
        # Do your stuff here.
    
        # Make sure to call the BaseCustomProcessor constructor
        # at the END of your constructor definition! This is
        # required to handle all the logging logic. 
        # super().__init__(**kwargs)
    
    def __call__(self, past_token_ids, logits):
        """
        In HuggingFace `transformers`, the `logits` passed into __call__
        are 2-dimension of shape (batch_size, vocab_size). However, as
        the pipeline is only ever run with a single batch, we may squeeze
        out the first dimension for easier logic. 
        """
        logits = logits.squeeze(0)

        # Do your stuff here. 

        ## Include the following to handle logging for analysis/plotting
        # if self.analysis_mode:
        #     self._write_log(
        #         my_kwarg,
        #         past_token_ids,
        #     )

        return logits.unsqueeze(0)
    
    def _write_log(self, **kwargs):
        """
        Write to the JSONL file at `self.log_file` using:

        out_data = {
            "interesting_datum": something_calculated_in__call__,
            "past_token_ids": past_token_ids.tolist()[0]
        }

        self.log_file.write(json.dumps(out_data) + "\n")
        self.log_file.flush()

        IMPORTANT: Store "past_token_ids": past_token_ids.tolist()[0]
        in your out_data in order to include the chat history in
        titles of plots generated by the analysis script. 
        """
        pass